{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d26a3b-1494-4ff7-96e8-2ad3fc26b89f",
   "metadata": {},
   "source": [
    "# Support Vector Machines - SVM\n",
    "\n",
    "SVM é um poderoso e versátil modelo de machine learning, capaz de realizar classificação linear e não linear, regressão e até mesmo detecção de outliers. É um dos modelos mais populares em Machine Learning e também um dos mais recomendados a se ter em sua caixa de ferramentas. SVMs são particularmente adequados para classificação de datasets complexos de tamanho pequeno ou médio. \n",
    "\n",
    "A ideia fundamental por trás dos SVMs é melhor explicada com imagens. Para isso, vamos usar o dataset iris já visto em Machine Learning I. Vamos importar os pacotes necessários, fazer o treino do modelo e plotar algumas imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a30b6-4ce3-4a19-8d15-5b96f9931575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28fa29-c94d-48ba-b3d1-00ceed354b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "setosa_or_versicolor = (y == 0) | (y == 1) #selecionamos apenas os dois tipos que temos interesse\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]\n",
    "\n",
    "# SVM Classifier model\n",
    "svm_clf = SVC(kernel=\"linear\", C=float(1000000000))\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e93458-8529-49ee-9887-f59c3311a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad models\n",
    "x0 = np.linspace(0, 5.5, 200)\n",
    "pred_1 = 5*x0 - 20\n",
    "pred_2 = x0 - 1.8\n",
    "pred_3 = 0.1 * x0 + 0.5\n",
    "\n",
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # a reta de nossa superfície de decisão tem como equacao w0*x0 + w1*x1 + b = 0, logo\n",
    "    # x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
    "    plt.plot(x0, gutter_down, \"k--\", linewidth=2)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(20,5), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(x0, pred_1, \"g--\", linewidth=2)\n",
    "plt.plot(x0, pred_2, \"m.\", linewidth=2)\n",
    "plt.plot(x0, pred_3, \"r-\", linewidth=2)\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_svc_decision_boundary(svm_clf, 0, 5.5)\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa28d6-c5ed-4605-9563-02152ee506c7",
   "metadata": {},
   "source": [
    "A Figura acima apresenta parte do dataset iris introduzido anteriormente. As duas classes claramente podem ser separadas usando uma linha reta (ou seja, são linearmente separáveis). \n",
    "\n",
    "O **plot da esquerda** apresenta as fronteiras de decisão de tres possíveis classificadores lineares. O modelo representado pela linha verde é ruim a ponto de não separar corretamente as classes. Os outros dois modelos funcionam corretamente, mas não conseguiriam generalizar de maneira correta. \n",
    "\n",
    "Em contraste, a linha sólida no **plot da direita** representa a fronteira de decisão de um classificador SVM. Esta linha não apenas separa as duas classes como também fica o mais longe possível das amostras de treinamento mais próximas. \n",
    "\n",
    "Assim, podemos pensar no SVM como ajustar a maior rua (representada pelas duas linhas pontilhadas) entre as classes. \n",
    "\n",
    "**Importante notar que adicionar mais instâncias \"fora da rua\" não afetará a fronteira de decisão, já que ela é totalmente determinada (ou suportada) pelas instâncias localizadas na margem da rua. Essas instâncias são denominadas *support vectors*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ef16c-3410-427d-ac23-5a371ede0bb3",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Warning!</font>\n",
    "\n",
    "SVMs são muito sensíveis à escala, como pode ser visto na imagem abaixo. No **plot da esquerda**, a escala vertical é muito maior que a escala horizontal, então a maior rua possível é próxima da horizontal. Depois de realizar *feature scaling* (**plot da direita**), a fronteira de decisão se torna muito melhor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e35a43-fb6d-4cd9-b8cc-2726c52fbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\n",
    "ys = np.array([0, 0, 1, 1])\n",
    "svm_clf = SVC(kernel=\"linear\", C=100)\n",
    "svm_clf.fit(Xs, ys)\n",
    "\n",
    "plt.figure(figsize=(9,2.7))\n",
    "plt.subplot(121)\n",
    "plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\n",
    "plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\n",
    "plot_svc_decision_boundary(svm_clf, 0, 6)\n",
    "plt.xlabel(\"$x_0$\", fontsize=20)\n",
    "plt.ylabel(\"$x_1$    \", fontsize=20, rotation=0)\n",
    "plt.title(\"Sem Normalização\", fontsize=16)\n",
    "plt.axis([0, 6, 0, 90])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(Xs)\n",
    "svm_clf.fit(X_scaled, ys)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\n",
    "plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\n",
    "plot_svc_decision_boundary(svm_clf, -2, 2)\n",
    "plt.xlabel(\"$x'_0$\", fontsize=20)\n",
    "plt.ylabel(\"$x'_1$  \", fontsize=20, rotation=0)\n",
    "plt.title(\"Com Normalização\", fontsize=16)\n",
    "plt.axis([-2, 2, -2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f4f119-7f6d-4e8d-9e2f-9f0b2780bc74",
   "metadata": {},
   "source": [
    "## Soft Margin Classification\n",
    "\n",
    "Se impormos que todas as amostras precisam estar fora da rua e do lado correto, temos o que chamamos de *hard margin classification*, que apresenta dois problemas principais:\n",
    "\n",
    "1. Funciona apenas com datasets linearmente separáveis\n",
    "2. É sensível a outliers\n",
    "\n",
    "A Figura abaixo mostra o dataset iris adicionando apenas 1 outlier em duas situações distintas: **na esquerda**, é impossível encontrar a margem; **na direita** a fronteira de decisão fica completamente diferente da apresentada acima e provavelmente irá falhar na generalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183cf9d-cf2e-4146-94e4-d640c344e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\n",
    "y_outliers = np.array([0, 0])\n",
    "Xo1 = np.concatenate([X, X_outliers[:1]], axis=0)\n",
    "yo1 = np.concatenate([y, y_outliers[:1]], axis=0)\n",
    "Xo2 = np.concatenate([X, X_outliers[1:]], axis=0)\n",
    "yo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n",
    "\n",
    "svm_clf2 = SVC(kernel=\"linear\", C=10**9)\n",
    "svm_clf2.fit(Xo2, yo2)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(20,5), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")\n",
    "plt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")\n",
    "plt.text(0.3, 1.0, \"Impossível!\", fontsize=24, color=\"red\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.annotate(\"Outlier\",\n",
    "             xy=(X_outliers[0][0], X_outliers[0][1]),\n",
    "             xytext=(2.5, 1.7),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "             fontsize=16,\n",
    "            )\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")\n",
    "plt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")\n",
    "plot_svc_decision_boundary(svm_clf2, 0, 5.5)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.annotate(\"Outlier\",\n",
    "             xy=(X_outliers[1][0], X_outliers[1][1]),\n",
    "             xytext=(3.2, 0.08),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "             fontsize=16,\n",
    "            )\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e292f4-a331-4b05-bd1e-ef677b71c8cb",
   "metadata": {},
   "source": [
    "Para evitar esses problemas, usamos um modelo mais flexível. O objetivo é **encontrar um bom equilíbrio entre manter a rua mais larga o possível e limitar violações da margem**. Isto é chamado *Soft Margin Classification*.\n",
    "\n",
    "Quando criamos um modelo com a Scikit-Learn, podemos especificar vários hiperparâmetros, entre eles o *C*. Essa parâmtro indica o quão rigoroso eu desejo que o modelo seja em relação a violações à margem, ou seja, a pontos que fiquem do lado oposto à margem encontrada para sua classe. Isto é importante pois em alguns casos pode ser que não seja possível encontrar uma margem que separe todos os pontos, ou, se essa margem existir, ela seja muito pequena, tornando a generalização do modelo para novos exemplos muito ruim. Um outro problema que pode ser causado com um hiperparâmetro muito rigoroso é gerar uma grande sensibilidade a outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be855041-a7ec-43aa-8990-355128608f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(C=0.1, kernel='linear')\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b738366-3951-4671-8e38-4949c75810c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,2.7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plot_svc_decision_boundary(svm_clf, 0, 5.5)\n",
    "plt.axis([0, 5.5, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47bd9e-5dd7-4025-a2ae-2540f9fa47e1",
   "metadata": {},
   "source": [
    "# SVM para Classificação Não-Linear\n",
    "\n",
    "Embora os classificadores SVM lineares sejam eficientes e funcionem bem em muitos casos, muitos datasets não são nem perto de serem linearmente separáveis. Uma abordagem para lidar com dataset não lineares é adicionar mais features, como as features polinomais. Em alguns casos, isso pode resultar em datasets linearmente separáveis. \n",
    "\n",
    "Considere o **plot da esquerda** na Figura abaixo. Ele representa um simples dataset com apenas uma feature, $x_1$. Este dataset não é linearmente separável. Mas se adicionarmos uma segunda feature $x_2 = (x_1)^2$ o resultado será um dataset linearmnete separável (**plot da direita**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6393f-ecf0-4a72-a65b-21392f212e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\n",
    "X2D = np.c_[X1D, X1D**2]\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.plot(X1D[:, 0][y==0], np.zeros(4), \"bs\")\n",
    "plt.plot(X1D[:, 0][y==1], np.zeros(5), \"g^\")\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "plt.axis([-4.5, 4.5, -0.2, 0.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], \"bs\")\n",
    "plt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], \"g^\")\n",
    "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "plt.ylabel(r\"$x_2$  \", fontsize=20, rotation=0)\n",
    "plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\n",
    "plt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\n",
    "plt.axis([-4.5, 4.5, -1, 17])\n",
    "\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca07774-669c-4c64-9978-160d3eab1c33",
   "metadata": {},
   "source": [
    "Para implementar essa ideia usando a SKLearn, podemos criar um *Pipeline* contendo três transformers: *PolynomialFeatures*, *StandardScaler* e *LinearSVC*. Veja o exemplo a abaixo, usando o dataset moons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea009d-f2ef-4f2a-9f0e-0d368c546d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c1cbf-0326-43c7-8d5b-11eccd5762e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209ffd3-ddef-40af-9174-938503eb6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec3e75-17ee-489f-8a2f-27fa4331bd7d",
   "metadata": {},
   "source": [
    "## Kernel Polinomial\n",
    "\n",
    "Adicionar features polinomais é simples de implementar e pode funcionar bem com vários algoritmos de machine learning. Dito isto, num baixo grau polinomial, este método não pode lidar com datasets muito complexos, e com um alto grau polinomial um número muito grande de features é criado, tornando o modelo muito lento. \n",
    "\n",
    "Felizmente, quando usamos SVM, podemos aplicar a técnica milagrosa chamada *kernel trick*, que possibilita obter o mesmo resultado como se você tivesse adicionado muitas features polinomais, mesmo com polinômios de graus muito altos, sem realmente ter adicionado elas. Assim, não há uma explosão combinatória do número de features pois, de fato, não adicionamos essas features. Vejamos o código para entender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa82917-ddab-4096-b6df-a162c7f38fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583875bf-a661-47a5-bec1-13149b0c06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly100_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n",
    "    ])\n",
    "poly100_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9867243-8352-4a29-b2d8-21c5dd454817",
   "metadata": {},
   "source": [
    "O código acima treina um SVM usando um kernel polinomial de grau 3, apresentado no **plot da esquerda**, e um SVM usando um kernel polinomial de grau 10, mostrado no **plot da direita**. O hiperparâmetro *coef0* controla o quanto o modelo é influenciado por polinômios de alto grau versus polinômios de baixo grau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055571e-9fcf-49f5-9334-6d85cd83cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
    "plt.title(r\"$d=3, r=1, C=5$\", fontsize=18)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
    "plt.title(r\"$d=10, r=100, C=5$\", fontsize=18)\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035132a2-b894-4736-bbc9-8f5f78722420",
   "metadata": {},
   "source": [
    "# Similarity Features\n",
    "\n",
    "Outra técnica para lidar com problemas não lineares é adicionar features calculadas usando uma função de similaridade, que mede o quanto cada instância se assemelha a um ponto de referência específico. Vamos retornar ao dataset 1D, visto acima, e adicionar dois pontos de referência a ele, $x_1=-2$ e $x_2=1$.\n",
    "\n",
    "Agora, vamos definir a função de similaridade *Gaussian Radial Basis Function (RBF)* com $\\gamma=0.3$:\n",
    "\n",
    "$\\phi\\gamma(x, \\ell) = exp(-\\gamma\\|x-\\ell\\|^2)$\n",
    "\n",
    "Esta é uma função em forma de sino que varia de 0 (muito distante do ponto de referência) até 1 (no ponto de referência). Agora estamos prontos para calcular as novas features. Por exemplo, veja a amostra $x_1=-1$: ela está localizada a uma distância 1 do primeiro ponto de referência e uma distância 2 do segundo ponto de referência. \n",
    "\n",
    "Dessa maneira, suas novas features serão $x_{1,1} =exp(-0.3\\times 1^2) \\approx 0.74$ e $x_{1,2} =exp(-0.3\\times 2^2) \\approx 0.30$. \n",
    "\n",
    "Como podemos ver, agora os dados são linearmente separáveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c06c3d-f894-4cf8-87df-dcb34cd15249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_rbf(x, landmark, gamma):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n",
    "\n",
    "gamma = 0.3\n",
    "\n",
    "x1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\n",
    "x2s = gaussian_rbf(x1s, -2, gamma)\n",
    "x3s = gaussian_rbf(x1s, 1, gamma)\n",
    "\n",
    "XK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]\n",
    "yk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "plt.figure(figsize=(10.5, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c=\"red\")\n",
    "plt.plot(X1D[:, 0][yk==0], np.zeros(4), \"bs\")\n",
    "plt.plot(X1D[:, 0][yk==1], np.zeros(5), \"g^\")\n",
    "plt.plot(x1s, x2s, \"g--\")\n",
    "plt.plot(x1s, x3s, \"b:\")\n",
    "plt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "plt.ylabel(r\"Similarity\", fontsize=14)\n",
    "plt.annotate(r'$\\mathbf{x}$',\n",
    "             xy=(X1D[3, 0], 0),\n",
    "             xytext=(-0.5, 0.20),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "             fontsize=18,\n",
    "            )\n",
    "plt.text(-2, 0.9, \"$x_2$\", ha=\"center\", fontsize=20)\n",
    "plt.text(1, 0.9, \"$x_3$\", ha=\"center\", fontsize=20)\n",
    "plt.axis([-4.5, 4.5, -0.1, 1.1])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], \"bs\")\n",
    "plt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], \"g^\")\n",
    "plt.xlabel(r\"$x_2$\", fontsize=20)\n",
    "plt.ylabel(r\"$x_3$  \", fontsize=20, rotation=0)\n",
    "plt.annotate(r'$\\phi\\left(\\mathbf{x}\\right)$',\n",
    "             xy=(XK[3, 0], XK[3, 1]),\n",
    "             xytext=(0.65, 0.50),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "             fontsize=18,\n",
    "            )\n",
    "plt.plot([-0.1, 1.1], [0.57, -0.1], \"r--\", linewidth=3)\n",
    "plt.axis([-0.1, 1.1, -0.1, 1.1])\n",
    "    \n",
    "plt.subplots_adjust(right=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ec65b-d323-402c-b136-694cc96bd45d",
   "metadata": {},
   "source": [
    "# Kernel RBF\n",
    "\n",
    "Assim como o método *Polynomial Features*, o método *Similarity Features* pode ser útil com qualquer algoritmo de Machine Learning, mas pode ser computacionalmente caro calcular todos as features adicionais, especialmente em grandes conjuntos de treinamento. Mais uma vez, o kernel trick faz sua mágica SVM, tornando possível obter um resultado semelhante como se você tivesse adicionado muitas features de similaridade. Veja o código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4da32b-8c72-41c2-a0aa-04a356199a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "    ])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac5a3d-bf1a-4222-9413-186bf9c38fb0",
   "metadata": {},
   "source": [
    "Este modelo é representado no canto inferior esquerdo da imagem abaixo. Os demais gráficos mostram modelos treinados com diferentes valores dos hiperparâmetros $\\gamma$ e $C$ . Aumentar o $\\gamma$ torna a curva em forma de sino mais estreita. Como resultado, o alcance de influência de cada instância é menor: o limite de decisão acaba sendo mais curvo em forma de sino: as instâncias têm um alcance de influência maior e a decisão irregular, oscilando em torno de instâncias individuais. Por outro lado, um valor pequeno de $\\gamma$ torna o limite mais suave. Então $\\gamma$ age como um hiperparâmetro de regularização: se o seu modelo estiver overfitado, você deve reduzi-lo; se estiver underfitado, você deve aumentá-lo (semelhante ao hiperparâmetro $C$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f4fb1-0daf-4ae1-a5ca-f20676bc7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "gamma1, gamma2 = 0.1, 5\n",
    "C1, C2 = 0.001, 1000\n",
    "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
    "\n",
    "svm_clfs = []\n",
    "for gamma, C in hyperparams:\n",
    "    rbf_kernel_svm_clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
    "        ])\n",
    "    rbf_kernel_svm_clf.fit(X, y)\n",
    "    svm_clfs.append(rbf_kernel_svm_clf)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n",
    "\n",
    "for i, svm_clf in enumerate(svm_clfs):\n",
    "    plt.sca(axes[i // 2, i % 2])\n",
    "    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n",
    "    gamma, C = hyperparams[i]\n",
    "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n",
    "    if i in (0, 1):\n",
    "        plt.xlabel(\"\")\n",
    "    if i in (1, 3):\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036b148-d8eb-4564-a17e-6e5334afaada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
